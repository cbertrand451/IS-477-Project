# Reproducible Workflow Instructions
This document describes all steps required to fully reproduce the project workflow from raw data acquisition to the end analysis. 

The workflow can be executed in two ways:

1. Using Snakemake   
2. Using the run_all.py script 

Both methods regenerate the entire project from scratch using the scripts stored in the scripts directory.

---

# Prerequisites

To repeat the workflow, the following software must be installed:

### Required:
- Python 3.10+  
- Conda environment or Miniconda  
- Required Python packages (requirements.txt)  
- Snakemake

### Installing requirements:
Inside your project environment:

pip install -r requirements.txt

---

# Workflow Overview

The workflow consists of the following automated stages:

Data Acquisition
- Fetch education dataset
- Compute a SHA-256 checksum for the manually 
downloaded income dataset
- Verify the integrity of both datasets

Data Loading
- Load both datasets into a SQLite database

Data Cleaning
- Convert types, remove invalid rows, standardize column names

Data Integration
- Create income-tier categories
- Merge education and income data based on those tiers

Analysis & Visualization
- Generate summary statistics
- Produce boxplots, bar charts, and histograms
- Export all results to the results/ folder

Each stage corresponds to a script in the scripts/ directory.

---

# Running the Workflow with Snakemake 

Snakemake executes each stage of the workflow automatically and guarantees correct ordering via file dependencies.

### Activate environment
In a conda environment, run the command:

conda activate snakemake

### Run workflow

From the project root run:

snakemake --cores 1


Snakemake will:
- Fetch data
- Compute checksums
- Verify integrity
- Load into database
- Clean the data
- Integrate datasets
- Run all analyses
- Save outputs in results

---

# Running the Workflow with run_all.py 

The entire workflow can be reproduced with a single python script.

The user should run:

python run_all.py

This script sequentially executes:

- fetch_education.py
- make_sha256.py
- load_to_db.py
- clean_data.py
- integrate_data.py
- analyze_data.py

---

# Required Input Files

The workflow requires only one manually downloaded dataset:

data/raw/income/counties_per_capita_income.csv

All other datasets are downloaded programmatically.

---

# Outputs Generated by the Workflow

The following files are created automatically:

Processed Database
- data/processed/education_income.db

Summary Statistics
- results/major_summary.csv
- results/county_summary.csv
- results/variability.txt

Visualizations
- results/boxplot_income_comparison.png
- results/majors_by_income_tier.png
- results/counties_by_income_tier.png
- results/income_histogram_variability.png